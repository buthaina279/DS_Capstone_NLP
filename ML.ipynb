{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4yOCQtnphgZkvG9bmT8m+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buthaina279/DS_Capstone_NLP/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hBLp2YgDuOE",
        "outputId": "0dc2dbc4-5e13-439d-acca-a911e6210ad4"
      },
      "source": [
        "# Import Libraries\n",
        "\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "from google.colab import drive  \n",
        "#import pycountry\n",
        "import re\n",
        "import string\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from PIL import Image\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "#from langdetect import detect\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siq9Tf95D-US",
        "outputId": "96eb63b9-cad9-4318-d216-b2a490a1c8a5"
      },
      "source": [
        "# Connect Google Drive to Colab\n",
        "drive.mount('/content/gdrive')\n",
        "# Create a variable to store the data path on your drive\n",
        "path = './gdrive/My Drive/datasets/twitter_analysis'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C96D4jgjED77"
      },
      "source": [
        "saudia_airlines = pd.read_csv(\"./gdrive/My Drive/datasets/twitter_analysis/saudia_airlines_classified.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "xFgcHpvLEkY-",
        "outputId": "48651c62-0eba-4102-fbc2-8eb19466cdd6"
      },
      "source": [
        "saudia_airlines.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>likes</th>\n",
              "      <th>retweeted</th>\n",
              "      <th>location</th>\n",
              "      <th>language</th>\n",
              "      <th>airline</th>\n",
              "      <th>Tweet_nonstop</th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>polarity</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1335393841810202624</td>\n",
              "      <td>2020-12-06 01:21:16</td>\n",
              "      <td>dear guest there is no flight from india to s...</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>Saudi Arabia</td>\n",
              "      <td>en</td>\n",
              "      <td>Saudi_airlines</td>\n",
              "      <td>['', 'dear', 'guest', 'flight', 'india', 'saud...</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.600</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>1335337522738720770</td>\n",
              "      <td>2020-12-05 21:37:29</td>\n",
              "      <td>hello dear guest we hope you check and be inf...</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>Saudi Arabia</td>\n",
              "      <td>en</td>\n",
              "      <td>Saudi_airlines</td>\n",
              "      <td>['', 'hello', 'dear', 'guest', 'hope', 'check'...</td>\n",
              "      <td>0.375</td>\n",
              "      <td>-0.125</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>29</td>\n",
              "      <td>1335334320014315521</td>\n",
              "      <td>2020-12-05 21:24:45</td>\n",
              "      <td>hello dear guest please provide us with the b...</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>Saudi Arabia</td>\n",
              "      <td>en</td>\n",
              "      <td>Saudi_airlines</td>\n",
              "      <td>['', 'hello', 'dear', 'guest', 'please', 'prov...</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.100</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>38</td>\n",
              "      <td>1335322063851167748</td>\n",
              "      <td>2020-12-05 20:36:03</td>\n",
              "      <td>dear guest there is no flight from india to s...</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>Saudi Arabia</td>\n",
              "      <td>en</td>\n",
              "      <td>Saudi_airlines</td>\n",
              "      <td>['', 'dear', 'guest', 'flight', 'india', 'saud...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>43</td>\n",
              "      <td>1335314759588122626</td>\n",
              "      <td>2020-12-05 20:07:02</td>\n",
              "      <td>hello dear guest you can follow up the flight...</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>Saudi Arabia</td>\n",
              "      <td>en</td>\n",
              "      <td>Saudi_airlines</td>\n",
              "      <td>['', 'hello', 'dear', 'guest', 'follow', 'flig...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "      <td>1335306576312356865</td>\n",
              "      <td>2020-12-05 19:34:31</td>\n",
              "      <td>dear guest there is no flight from india to s...</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>Saudi Arabia</td>\n",
              "      <td>en</td>\n",
              "      <td>Saudi_airlines</td>\n",
              "      <td>['', 'dear', 'guest', 'flight', 'india', 'saud...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>51</td>\n",
              "      <td>1335304974679613443</td>\n",
              "      <td>2020-12-05 19:28:09</td>\n",
              "      <td>hello dear valued guest once the confirmation...</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>Saudi Arabia</td>\n",
              "      <td>en</td>\n",
              "      <td>Saudi_airlines</td>\n",
              "      <td>['', 'hello', 'dear', 'valued', 'guest', 'conf...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>53</td>\n",
              "      <td>1335276146653589507</td>\n",
              "      <td>2020-12-05 17:33:36</td>\n",
              "      <td>dear guest there is no flight from india to s...</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>Saudi Arabia</td>\n",
              "      <td>en</td>\n",
              "      <td>Saudi_airlines</td>\n",
              "      <td>['', 'dear', 'guest', 'flight', 'india', 'saud...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>55</td>\n",
              "      <td>1335275872295804928</td>\n",
              "      <td>2020-12-05 17:32:30</td>\n",
              "      <td>dear guest there is no flight from india to s...</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>Saudi Arabia</td>\n",
              "      <td>en</td>\n",
              "      <td>Saudi_airlines</td>\n",
              "      <td>['', 'dear', 'guest', 'flight', 'india', 'saud...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>57</td>\n",
              "      <td>1335266364739694593</td>\n",
              "      <td>2020-12-05 16:54:44</td>\n",
              "      <td>hello dear guest imran please provide us with...</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>Saudi Arabia</td>\n",
              "      <td>en</td>\n",
              "      <td>Saudi_airlines</td>\n",
              "      <td>['', 'hello', 'dear', 'guest', 'imran', 'pleas...</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.100</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  ...  polarity sentiment\n",
              "0           0            14  ...     0.600  Positive\n",
              "1           1            23  ...    -0.125  Negative\n",
              "2           2            29  ...     0.100  Positive\n",
              "3           3            38  ...     0.000   Neutral\n",
              "4           4            43  ...     0.000   Neutral\n",
              "5           5            50  ...     0.000   Neutral\n",
              "6           6            51  ...     0.000   Neutral\n",
              "7           7            53  ...     0.000   Neutral\n",
              "8           8            55  ...     0.000   Neutral\n",
              "9           9            57  ...     0.100  Positive\n",
              "\n",
              "[10 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYMS7ZgKaYqm"
      },
      "source": [
        "#preproccesing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dBL1l0Aac9D"
      },
      "source": [
        "Dealing with dates and time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oacRo_7uEs3T"
      },
      "source": [
        "#Extracting Features from Cleaned Tweets\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD94AIrcRWX0"
      },
      "source": [
        "#Bag-of-Words Features\n",
        "\n",
        "Bag-of-Words is a method to represent text into numerical features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYDErjcPSB7g"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "# bag-of-words feature matrix\n",
        "bow = bow_vectorizer.fit_transform(saudia_airlines['Tweet_nonstop'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNH1b-s0SfVW"
      },
      "source": [
        "bow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rATRrAm3RulO"
      },
      "source": [
        "#TF-IDF Features\n",
        "\n",
        "This is another method which is based on the frequency method but it is different to the bag-of-words approach in the sense that it takes into account, not just the occurrence of a word in a single document (or tweet) but in the entire corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p469J3f1Erst"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "# TF-IDF feature matrix\n",
        "tfidf = tfidf_vectorizer.fit_transform(saudia_airlines['Tweet_nonstop'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "697UvFrzS1Ez"
      },
      "source": [
        "#Model calibration\n",
        "\n",
        "https://towardsdatascience.com/classifier-calibration-7d0be1e05452"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pprTGJHS1Tb"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = saudia_airlines.iloc[:,:-1].values #all predictor columns without the target\n",
        "y = saudia_airlines.iloc[:,-1].values #last column (target)\n",
        "\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify=y, random_state=123)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH-gU32-X4VF"
      },
      "source": [
        "Next, we instantiate two classifiers to compare; a simple logistic regression model and an implementation of Support Vector Machines provided by scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bITDsHjX5aq"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "lgr = LogisticRegression(C=1, solver='lbfgs')\n",
        "svc = SVC(max_iter=10000, probability=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8vaeLQFYOKW"
      },
      "source": [
        "Finally, we fit the classifiers to our training data and compute our predictions on the test data set. Specifically for the SVM, to get the probabilities for the positive class we need to know how the decision function separates the test samples, and normalize the results to be between `0` and `1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "5WeepN0xYDyt",
        "outputId": "68d51516-0435-4ac7-aa24-0691f7b23fdb"
      },
      "source": [
        "probs_lgr = lgr.fit(X_train, y_train).predict_proba(X_test)[:,1]\n",
        "preds_svc = svc.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "probs_svc = svc.decision_function(X_test)\n",
        "probs_svc = (probs_svc - probs_svc.min()) / (probs_svc.max() - probs_svc.min())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d248b6017aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobs_lgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpreds_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprobs_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprobs_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprobs_svc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprobs_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprobs_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprobs_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1527\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2020-11-01 16:26:19'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUeLOvztYRBo"
      },
      "source": [
        "Let us now plot the Kernel Density Estimation for the two classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EPMSXBBYRyJ"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.kdeplot(probs_lgr, label='Logistic regression')\n",
        "sns.kdeplot(preds_svc, label='SVM')\n",
        "plt.title(\"Probability Density Plot for 2 Classifiers\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyWwbreAYcR1"
      },
      "source": [
        "let us check the AUC-ROC curve for the two binary classifiers, but this time using the probabilities we calculated for the SVM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3RfeO8iYcyf"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "\n",
        "pred = probs_lgr\n",
        "label = y_test\n",
        "fpr, tpr, thresh = metrics.roc_curve(label, pred)\n",
        "auc = metrics.roc_auc_score(label, pred)\n",
        "plt.plot(fpr, tpr, label=f'Logistic regression, auc = {str(round(auc,3))}')\n",
        "\n",
        "pred = probs_svc\n",
        "fpr, tpr, thresh = metrics.roc_curve(label, pred)\n",
        "auc = metrics.roc_auc_score(label, pred)\n",
        "plt.plot(fpr, tpr, label=f'SVC, auc = {str(round(auc,3))}')\n",
        "\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.title(\"AUC-ROC for two models\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh2qqzl7Yk6H"
      },
      "source": [
        "To plot the calibration curve of each classifier we define a utility function like the one below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV_AJMbUYn4c"
      },
      "source": [
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "\n",
        "def plot_calibration_curve(name, fig_index, probs):\n",
        "    \"\"\"Plot calibration curve for est w/o and with calibration. \"\"\"\n",
        "\n",
        "    fig = plt.figure(fig_index, figsize=(10, 10))\n",
        "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
        "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
        "    \n",
        "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
        "    \n",
        "    frac_of_pos, mean_pred_value = calibration_curve(y_test, probs, n_bins=10)\n",
        "\n",
        "    ax1.plot(mean_pred_value, frac_of_pos, \"s-\", label=f'{name}')\n",
        "    ax1.set_ylabel(\"Fraction of positives\")\n",
        "    ax1.set_ylim([-0.05, 1.05])\n",
        "    ax1.legend(loc=\"lower right\")\n",
        "    ax1.set_title(f'Calibration plot ({name})')\n",
        "    \n",
        "    ax2.hist(probs, range=(0, 1), bins=10, label=name, histtype=\"step\", lw=2)\n",
        "    ax2.set_xlabel(\"Mean predicted value\")\n",
        "    ax2.set_ylabel(\"Count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwzJQIZuYr3e"
      },
      "source": [
        "# plot calibration curve for logistic regression\n",
        "plot_calibration_curve(\"Logistic regression\", 1, probs_lgr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jZllfTDYxH7"
      },
      "source": [
        "# plot calibration curve for the SVM\n",
        "plot_calibration_curve(\"SVM\", 1, probs_svc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbxdScdwY13i"
      },
      "source": [
        "#Calibrating the model\n",
        "\n",
        "The two most popular methods of calibrating a machine learning model are the isotonic and Platt's method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZJ0DGFoY2sE"
      },
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "\n",
        "lgr = LogisticRegression(C=1, solver='lbfgs')\n",
        "svc = SVC(max_iter=10000, probability=True)\n",
        "\n",
        "platts_scaling = CalibratedClassifierCV(svc, cv=2, method='sigmoid')\n",
        "platts_scaling.fit(X_train, y_train)\n",
        "calibrated_probs = platts_scaling.predict_proba(X_test)[:,1]\n",
        "\n",
        "plot_calibration_curve(\"SVM\", 3, calibrated_probs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}